{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AAZyn5TIYG3zWdTNtLGBLo2O8QokPRh8",
      "authorship_tag": "ABX9TyOZIsa2QeP1ymnEIcaB8L3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zilliax-Barry/NTU-AI6122/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zzpKPJeVJeJ7",
        "outputId": "fef330e5-15eb-4388-d08a-fe83ca97860e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjr-pHTRWpDS",
        "outputId": "678aca64-8eb6-4731-c9b9-714663f91d19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "import datetime\n",
        "#from helpers.duallogger import loggersetup\n",
        "#from helpers.filehelper import is_not_empty_file_exists, write_to_file, load_from_file\n",
        "import logging\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import heapq\n",
        "import collections\n",
        "import operator\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "Bcyf7TleXkCv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Process"
      ],
      "metadata": {
        "id": "pVDkK6vBZhZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.json import json_normalize\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import json\n",
        "\n",
        "count = 0\n",
        "# 'reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',  'overall', 'summary', 'unixReviewTime', 'reviewTime'\n",
        "reviewerID = []\n",
        "asin = []\n",
        "reviewerName = []\n",
        "helpful = []\n",
        "reviewText = []\n",
        "overall = []\n",
        "summary = []\n",
        "unixReviewTime = []\n",
        "reviewTime = []\n",
        "\n",
        "with open('./Grocery_and_Gourmet_Food_5.json') as f:\n",
        "  for line in f:\n",
        "    dicts= json.loads(line.strip())\n",
        "    # print(dicts)\n",
        "    count = count + 1\n",
        "    if 'reviewerID' in dicts:\n",
        "      reviewerID.append(dicts['reviewerID'])\n",
        "    else:\n",
        "      reviewerName.append('noID')  \n",
        "    if 'reviewerName' in dicts:\n",
        "      reviewerName.append(dicts['reviewerName'])\n",
        "    else:\n",
        "      reviewerName.append('noName')  \n",
        "    if 'reviewText' in dicts:\n",
        "      reviewText.append(dicts['reviewText'])\n",
        "    else:\n",
        "      reviewText.append('noName')  \n",
        "\n",
        "print(count)  \n",
        "\n"
      ],
      "metadata": {
        "id": "jsrzpYjWoXvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6936d5-4856-4d43-a571-cd98e690c203"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the nlp object\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDW-2w2NCT21",
        "outputId": "0a7603bc-9cd2-47ff-cb37-71e6748df653"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process the reviewerText\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords_nltk = stopwords.words('english')\n",
        "stopwords_spacy = list(STOP_WORDS)\n",
        "stopwords_spacy.append('\\n')\n",
        "stopwords = stopwords_nltk + list(set(stopwords_spacy) - set(stopwords_nltk))\n",
        "\n",
        "print(\"sw nltk: \", len(stopwords_nltk))\n",
        "print(\"sw spacy: \", len(stopwords_spacy))\n",
        "print(\"combined: \", len(stopwords))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWMBqZ4IFjoY",
        "outputId": "329d57d9-4360-4f6e-af2a-7b2cc644ffa5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sw nltk:  179\n",
            "sw spacy:  327\n",
            "combined:  383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process the reviewerText\n",
        "global prenlp\n",
        "prenlp = spacy.load(\"en_core_web_sm\", disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
        "prenlp.add_pipe('sentencizer')\n",
        "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©.'\n",
        "extractText = []\n",
        "\n",
        "for i in range(len(reviewText)):\n",
        "  review = prenlp(reviewText[i])\n",
        "  # POS tagging & tokenize\n",
        "  sentence = nlp(review.text, disable=['parser', 'ner'])\n",
        "  tokens = [tok.lemma_.lower().strip() for tok in sentence if tok.lemma_ != '-PRON-']\n",
        "  tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
        "  tokens = ' '.join(tokens)\n",
        "  extractText.append(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "L1UBF5MNMU9x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractSentence = []\n",
        "for i in range(len(reviewText)):\n",
        "  doc = prenlp(reviewText[i]) \n",
        "  extractSentence.append([])\n",
        "  for idx, sentence in enumerate(doc.sents):\n",
        "      extractSentence[i].append(sentence.text)      "
      ],
      "metadata": {
        "id": "COeb4NISiMWB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviewText[300])\n",
        "print(extractText[300])\n",
        "print(extractSentence[300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIGz-vrzmtJv",
        "outputId": "8c88f05b-b005-46d8-cc14-e52e9ede5a0d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this ghost combo sauce hits different areas of the mouth than the red/orange/yellow/chocolate Habanerohighly recommended for a spice lovers aresenalfor best kick results combine with another multi-Habanero saucemade in costa rica\n",
            "ghost combo sauce hit different area mouth red orange yellow chocolate habanerohighly recommend spice lover aresenalfor good kick result combine multi habanero saucemade costa rica\n",
            "['this ghost combo sauce hits different areas of the mouth than the red/orange/yellow/chocolate Habanerohighly recommended for a spice lovers aresenalfor best kick results combine with another multi-Habanero saucemade in costa rica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review Summarizer"
      ],
      "metadata": {
        "id": "d8LmRD6vsWoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collection Frequency\n",
        "word_token = {}\n",
        "eT_join = []\n",
        "token = []\n",
        "\n",
        "for i in range(len(extractText)):\n",
        "  eT_join = extractText[i]\n",
        "  # eT_join the text in the ith reviewText\n",
        "  # print(eT_join)\n",
        "  token = eT_join.split(' ')\n",
        "  \n",
        "  for tk in token:\n",
        "    #print('============')\n",
        "    #print(tk)\n",
        "    if tk not in '.':\n",
        "      #print(tk)\n",
        "      if tk not in word_token:\n",
        "        word_token[tk] = 1\n",
        "      else:\n",
        "        word_token[tk] += 1\n",
        "\n",
        "print(word_token)\n",
        "\n"
      ],
      "metadata": {
        "id": "H-oPHxSEhshM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(a, b):\n",
        "  return dot(a, b)/(norm(a)*norm(b))"
      ],
      "metadata": {
        "id": "m7uqkFyWkSHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Space Summarizer"
      ],
      "metadata": {
        "id": "WvsdlQjtmuJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(prod_reviews, index, num_of_docs, word_doc_frequencies, num_of_sentences):\n",
        "    processedSentences = eval(prod_reviews['processedSentences'][index])\n",
        "    reviewSentences = eval(prod_reviews['reviewSentences'][index])\n",
        "\n",
        "    ## generate word frequencies in cleaned\n",
        "    cleaned_reviews = ' '.join(str(x) for x in processedSentences) \n",
        "    word_frequencies = _word_frequency(cleaned_reviews)\n",
        "\n",
        "    ## generate review vector of len of word frequencies\n",
        "    vocab = len(word_frequencies)\n",
        "    # log.debug(\"vocab: %i\" % vocab)\n",
        "    review_tf_vec = np.zeros(len(word_frequencies))\n",
        "    for idx, word in enumerate(word_frequencies):\n",
        "        tfidf_value = (1 + np.log(word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n",
        "        review_tf_vec[idx] = tfidf_value\n",
        "\n",
        "    # print(review_tf_vec)\n",
        "    # log.debug(len(review_tf_vec))\n",
        "\n",
        "    ## generate sentences\n",
        "    # log.debug(\"num of sentences: %i\" % len(processedSentences))\n",
        "    # log.debug(\"sentences: %s\" % processedSentences)\n",
        "\n",
        "    ## generate sentence vectors of len of word frequencies\n",
        "    sent_tf_vecs = []\n",
        "    for sentence in processedSentences:\n",
        "        sent_tf_vec = np.zeros(len(word_frequencies))\n",
        "        sent_word_frequencies = _word_frequency(sentence)\n",
        "        for idx, word in enumerate(sent_word_frequencies):\n",
        "            if word in word_frequencies.keys():\n",
        "                idx = list(word_frequencies.keys()).index(word)\n",
        "                tfidf_value = (1 + np.log(sent_word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n",
        "                sent_tf_vec[idx] = tfidf_value\n",
        "        sent_tf_vecs.append(sent_tf_vec)\n",
        "        # print('shape', sent_tf_vec.shape)\n",
        "\n",
        "    ## compare cossim between review vector and sentence vectors\n",
        "    cosine_similarity = []\n",
        "    for sent_tf_vec in sent_tf_vecs:\n",
        "        cosine_similarity.append(cos_sim(review_tf_vec, sent_tf_vec))\n",
        "\n",
        "    highest_cos_idx = np.argsort(cosine_similarity)[-num_of_sentences:]\n",
        "    # log.debug(\"highest_cos_idx: %s\" % str(highest_cos_idx))\n",
        "\n",
        "    ## original sentences\n",
        "    # log.debug(\"original_sentences: %s\" % reviewSentences)\n",
        "    # log.debug(\"num of ori sentences: %s\" % len(reviewSentences))\n",
        "    summary_sentences = []\n",
        "    for idx in highest_cos_idx:\n",
        "        # log.debug(\"idx: %i ori: %s cleaned: %s\" % (idx, reviewSentences[idx], processedSentences[idx]))\n",
        "        # log.debug(\"cos_vec: %s\" % str(sent_tf_vecs[idx]))\n",
        "        summary_sentences.append(reviewSentences[idx])\n",
        "\n",
        "    return \" \".join(summary_sentences), summary_sentences"
      ],
      "metadata": {
        "id": "EJ-fMTjFkTSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "KMeans Summarizer"
      ],
      "metadata": {
        "id": "u59wHH93nlA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### Kmeans algorithm ################\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def kmeans_summary(prod_reviews, index, num_of_docs, word_doc_frequencies, num_of_sentences):\n",
        "\n",
        "    processedSentences = eval(prod_reviews['processedSentences'][index])\n",
        "    reviewSentences = eval(prod_reviews['reviewSentences'][index])\n",
        "\n",
        "    ## generate word frequencies in cleaned\n",
        "    cleaned_reviews = ' '.join(str(x) for x in processedSentences) \n",
        "    word_frequencies = _word_frequency(cleaned_reviews)\n",
        "\n",
        "    sent_tf_vecs = []\n",
        "    for sentence in processedSentences:\n",
        "        sent_tf_vec = np.zeros(len(word_frequencies))\n",
        "        sent_word_frequencies = _word_frequency(sentence)\n",
        "        for idx, word in enumerate(sent_word_frequencies):\n",
        "            if word in word_frequencies.keys():\n",
        "                idx = list(word_frequencies.keys()).index(word)\n",
        "                tfidf_value = (1 + np.log(sent_word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n",
        "                sent_tf_vec[idx] = tfidf_value\n",
        "        sent_tf_vecs.append(sent_tf_vec)\n",
        "\n",
        "    est = KMeans(n_clusters=num_of_sentences, random_state=0).fit(np.array(sent_tf_vecs))\n",
        "\n",
        "    closest_centers_idx = []\n",
        "    indexes = np.arange(len(sent_tf_vecs))\n",
        "    cluster_ids = est.labels_\n",
        "    centroids = est.cluster_centers_\n",
        "\n",
        "    # calculate each sentence's dist from its corresponding centroid \n",
        "    dist_from_centroids = []\n",
        "    for idx in range(len(sent_tf_vecs)):\n",
        "        dist = euclidean_distances(sent_tf_vecs[idx].reshape(1,-1), centroids[cluster_ids[[idx]], :])\n",
        "        dist_from_centroids.append(dist[0][0])\n",
        "\n",
        "    dist_arr = np.concatenate([np.array([dist_from_centroids]), cluster_ids.reshape(1,-1), indexes.reshape(1,-1)], axis = 0 )\n",
        "    dist_arr = dist_arr.T\n",
        "\n",
        "    # find sentences with shortest dist to centroids\n",
        "    for cluster_id in range(len(centroids)):\n",
        "        dist_arr_id = dist_arr[dist_arr[:, 1] == cluster_id]\n",
        "        row_idx = dist_arr_id[:, 0].argmin()\n",
        "        sent_idx = dist_arr_id[row_idx][-1]\n",
        "        closest_centers_idx.append(sent_idx)\n",
        "    closest_centers_idx = sorted(closest_centers_idx)\n",
        "    # print('SELECTED SENT index:', closest_centers_idx)\n",
        "\n",
        "    # print('\\n')\n",
        "    # print('[ORIGINAL]:')\n",
        "    # for sent in reviewSentences:\n",
        "    #     print(sent)\n",
        "\n",
        "    #Comparing against vector space model\n",
        "    # _, vec_sent = generate_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=num_of_sentences)\n",
        "    # print('\\n')\n",
        "    # print('[SUMMARY Vector Space]:')\n",
        "    # for i in vec_sent:\n",
        "    #     print(i)\n",
        "\n",
        "    summary_sent = []\n",
        "    # print('\\n')\n",
        "    # print('[SUMMARY Kmeans]:')\n",
        "    for idx in closest_centers_idx:\n",
        "        # print(reviewSentences[int(idx)])\n",
        "        summary_sent.append(reviewSentences[int(idx)])\n",
        "    \n",
        "    return summary_sent"
      ],
      "metadata": {
        "id": "FwsEdC2hnlfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structured POS Summarizer"
      ],
      "metadata": {
        "id": "uXOXBFqYnoHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############preprocessing for stru pos summary#####################################\n",
        "\n",
        "data = pd.read_json('CellPhoneReview.json', lines=True)\n",
        "prod_ids  = data['asin'].values\n",
        "reviewTexts  = data['reviewText'].values\n",
        "unique_id = data['asin'].unique()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner', 'tagger'])\n",
        "sentnlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©'\n",
        "\n",
        "def cleanup_text(text, stopwords, punc):\n",
        "    texts = []\n",
        "    global sentnlp\n",
        "    doc = sentnlp(text) # only do tokenization and pos tagging\n",
        "    for sentence in doc.sents:\n",
        "        sen_text = nlp(sentence.text, disable=['parser', 'ner'])\n",
        "        tokens = [tok.lemma_.lower().strip() for tok in sen_text if tok.lemma_ != '-PRON-']\n",
        "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punc]\n",
        "        tokens = ' '.join(tokens)\n",
        "        texts.append(tokens)\n",
        "    return str(texts)\n",
        "\n",
        "def get_pos_tag(sent, noun_dict):\n",
        "    '''identify nouns and adjs in each sent'''\n",
        "\n",
        "    nouns = []\n",
        "    adj = []\n",
        "    for tok in sent:\n",
        "#         print(tok, tok.pos_, tok.lemma_)\n",
        "        if str(tok.pos_) == 'NOUN':\n",
        "            try:\n",
        "                noun_dict[str(tok.lemma_)] += 1\n",
        "            except:\n",
        "                noun_dict[str(tok.lemma_)] = 1\n",
        "            nouns.append(str(tok.lemma_))\n",
        "        elif str(tok.pos_) == 'ADJ':\n",
        "            adj.append(str(tok.lemma_))\n",
        "        \n",
        "    return nouns, adj"
      ],
      "metadata": {
        "id": "lXvYs6yOnn8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_idf(sentences):\n",
        "    \"get tf-idf vectors for sentences\"\n",
        "    num_of_docs = len(unique_id)\n",
        "    sent_tf_vecs = []\n",
        "    for sentence in sentences:\n",
        "        sent_tf_vec = np.zeros(len(word_frequencies))\n",
        "        sent_word_frequencies = _word_frequency(sentence)\n",
        "        for idx, word in enumerate(sent_word_frequencies):\n",
        "            if (word in word_doc_frequencies):\n",
        "              idx = all_words_ls.index(word)\n",
        "              tfidf_value = (1 + np.log(sent_word_frequencies[word])) * np.log(num_of_docs/word_doc_frequencies[word]) \n",
        "              sent_tf_vec[idx] = tfidf_value\n",
        "        sent_tf_vecs.append(sent_tf_vec)\n",
        "        \n",
        "    return sent_tf_vecs\n",
        "\n",
        "def get_center_vec(sent_tf_vecs):\n",
        "    \"get central sentence representation\"\n",
        "    est = KMeans(n_clusters=1, random_state=0).fit(np.array(sent_tf_vecs))\n",
        "\n",
        "    closest_centers_idx = []\n",
        "    indexes = np.arange(len(sent_tf_vecs))\n",
        "    cluster_ids = est.labels_\n",
        "    centroids = est.cluster_centers_\n",
        "\n",
        "    # calculate each sentence's dist from its corresponding centroid \n",
        "    dist_from_centroids = []\n",
        "    for idx in range(len(sent_tf_vecs)):\n",
        "        dist = euclidean_distances(sent_tf_vecs[idx].reshape(1,-1), centroids[cluster_ids[[idx]], :])\n",
        "        dist_from_centroids.append(dist[0][0])\n",
        "\n",
        "    dist_arr = np.concatenate([np.array([dist_from_centroids]), cluster_ids.reshape(1,-1), indexes.reshape(1,-1)], axis = 0 )\n",
        "    dist_arr = dist_arr.T\n",
        "\n",
        "    # find sentences with shortest dist to centroids\n",
        "    for cluster_id in range(len(centroids)):\n",
        "        dist_arr_id = dist_arr[dist_arr[:, 1] == cluster_id]\n",
        "        row_idx = dist_arr_id[:, 0].argmin()\n",
        "        sent_idx = dist_arr_id[row_idx][-1]\n",
        "        closest_centers_idx.append(sent_idx)\n",
        "\n",
        "    print('SELECTED SENT index:', closest_centers_idx)\n",
        "\n",
        "    best_vec = sent_tf_vecs[int(closest_centers_idx[0])]\n",
        "    \n",
        "    return best_vec, int(closest_centers_idx[0])\n",
        "\n",
        "def uncapitalize(s):\n",
        "  if len(s) > 0:\n",
        "    s = s[0].lower() + s[1:]\n",
        "  return s"
      ],
      "metadata": {
        "id": "zKd1WNXCnt8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 5\n",
        "print(\"INDEX: \", index)\n",
        "\n",
        "chosen_sent_idx = []\n",
        "\n",
        "ori_sent = []\n",
        "processed_sent = []\n",
        "last_sent = []\n",
        "nouns_master = []\n",
        "adj_master = []\n",
        "len_adj = []\n",
        "\n",
        "uni_id = unique_id[index]\n",
        "\n",
        "noun_dict = dict()\n",
        "for idx in range(len(prod_ids)):\n",
        "    if prod_ids[idx] == uni_id:\n",
        "        text = reviewTexts[idx]\n",
        "        sent_master = []\n",
        "        doc = nlp(text)\n",
        "        for sent in doc.sents:\n",
        "#             print(sent)\n",
        "            ori_sent.append(str(sent))\n",
        "            clean_sent = cleanup_text(str(sent),stopwords, punctuations)\n",
        "            processed_sent.append(eval(clean_sent)[0])\n",
        "            nouns_list, adj_list = get_pos_tag(sent, noun_dict)\n",
        "            nouns_master.append(nouns_list)\n",
        "            adj_master.append(adj_list)\n",
        "            len_adj.append(len(adj_list))\n",
        "            last_sent.append(0)\n",
        "#         print('\\n')\n",
        "        last_sent[-1] = 1\n",
        "    else:\n",
        "        continue\n",
        "        \n",
        "sentence_frame = pd.DataFrame({'ori_sent': ori_sent, 'processed_sent':processed_sent, 'nouns':nouns_master,\n",
        "                 'adj':adj_master, 'len_adj': len_adj, 'last_sent':last_sent, 'idx_sent': np.arange(len(ori_sent))})\n",
        "                              \n",
        "                              \n",
        "# get word freq of all words in reviews\n",
        "cleaned_reviews = ' '.join(str(x) for x in list(sentence_frame['processed_sent'].values)) \n",
        "word_frequencies = _word_frequency(cleaned_reviews)\n",
        "all_words_ls = list(word_frequencies.keys())\n",
        "num_of_sentences = min(len(sentence_frame['processed_sent']), 5)\n",
        "                      \n",
        "########################### GET  1st SENTENCE (best sent that represents most occuring noun) #############################################\n",
        "# find sentences with the most occuring noun\n",
        "max_noun = max(noun_dict.items(), key=operator.itemgetter(1))[0]\n",
        "clean_text_max_nouns = []\n",
        "max_nouns_idx = []\n",
        "for idx in range(len(sentence_frame)):\n",
        "    if max_noun in sentence_frame['nouns'][idx]:\n",
        "        clean_text_max_nouns.append(sentence_frame['processed_sent'][idx])\n",
        "        max_nouns_idx.append(sentence_frame['idx_sent'][idx])\n",
        "        \n",
        "print('max_noun', max_noun)\n",
        "                              \n",
        "# construct tfidf of sentences with max noun\n",
        "max_noun_tf_vecs = get_tf_idf(clean_text_max_nouns)\n",
        "                              \n",
        "#get central rep sent for max noun\n",
        "bext_noun_vec, best_noun_sent_idx = get_center_vec(max_noun_tf_vecs)\n",
        "chosen_idx = max_nouns_idx[best_noun_sent_idx]\n",
        "chosen_sent_idx.append(chosen_idx)\n",
        "num_of_sentences = num_of_sentences - 1                              \n",
        "\n",
        "################################################### GET  2nd and 3rg SENTENCE #############################################\n",
        "#############################################sents that contain adj and are similar to 1st sent###############################\n",
        "# get best adj sents\n",
        "clean_text_best_adjs = []\n",
        "best_adjs_idx = []\n",
        "for idx in range(len(sentence_frame)):\n",
        "    if idx not in chosen_sent_idx and len(sentence_frame['adj'][idx]) >= 1 :\n",
        "        clean_text_best_adjs.append(sentence_frame['processed_sent'][idx])\n",
        "        best_adjs_idx.append(sentence_frame['idx_sent'][idx])\n",
        "\n",
        "# construct tfidf of sentences with best adj\n",
        "best_adj_tf_vecs = get_tf_idf(clean_text_best_adjs)\n",
        "                              \n",
        "# compute cos sim between best_adj_tf_vec and max_noun_vec, also compute mean tfidf score for each sent\n",
        "adj_sim_dict = dict()\n",
        "adj_tfidf_dict = dict()\n",
        "for idx in range(len(best_adj_tf_vecs)):\n",
        "    sim = cos_sim(best_adj_tf_vecs[idx], bext_noun_vec)\n",
        "    adj_sim_dict[sim] = best_adjs_idx[idx]\n",
        "    tfidf_score = np.mean(best_adj_tf_vecs[idx])\n",
        "    adj_tfidf_dict[tfidf_score] = best_adjs_idx[idx]\n",
        "\n",
        "#get idx of 2 sents with best scores  \n",
        "for i in range(2):\n",
        "    if num_of_sentences > 0 and len(adj_sim_dict) > 0:\n",
        "      key = sorted(adj_sim_dict.keys())[-(i+1)]\n",
        "      chosen_sent_idx.append(adj_sim_dict[key])\n",
        "      num_of_sentences = num_of_sentences - 1 \n",
        "                             \n",
        "################################################### GET  4th SENTENCE based on pure tfidf score #############################################\n",
        "################################################### This is to add some diversity/details to the summary###################################\n",
        "while num_of_sentences > 0 and len(adj_tfidf_dict) > 0:\n",
        "    i = -1\n",
        "    key = sorted(adj_tfidf_dict.keys())[i]\n",
        "    idx = adj_tfidf_dict[key]\n",
        "    if idx in chosen_sent_idx:\n",
        "        i -= 1\n",
        "    else:\n",
        "        chosen_sent_idx.append(idx)\n",
        "        break\n",
        "################################################### GET LAST SENTENCE #############################################\n",
        "################################################### Most representative concluding sentence#######################################\n",
        "\n",
        "# get last sents\n",
        "last_sents = []\n",
        "last_sent_idx = []\n",
        "for idx in range(len(sentence_frame)):\n",
        "    if idx not in chosen_sent_idx and sentence_frame['last_sent'][idx] == 1 :\n",
        "        last_sents.append(sentence_frame['processed_sent'][idx])\n",
        "        last_sent_idx.append(sentence_frame['idx_sent'][idx])\n",
        "                              \n",
        "#get central rep sent for best last sentence\n",
        "last_sents_tf_vecs = get_tf_idf(last_sents)\n",
        "_, best_last_sent_idx = get_center_vec(last_sents_tf_vecs)\n",
        "chosen_idx = last_sent_idx[best_last_sent_idx]\n",
        "chosen_sent_idx.append(chosen_idx)\n",
        "                              \n",
        "print('\\n')\n",
        "print('[ORIGINAL]:')\n",
        "max_ori = 100\n",
        "full_summary = []\n",
        "for i in sentence_frame['ori_sent'].values:\n",
        "  if max_ori > 0:\n",
        "      print(i.strip() + \" \")\n",
        "      full_summary.append(i.strip() + \" \")\n",
        "      max_ori = max_ori - 1\n",
        "\n",
        "print('\\n')\n",
        "print('[ORIGINAL ONELINE]:')\n",
        "print(\"\".join(full_summary))\n",
        "\n",
        "num_of_sentences = min(len(sentence_frame['processed_sent']), 5)\n",
        "_, summary_vector_space = generate_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=num_of_sentences)\n",
        "summary_kmeans = kmeans_summary(prod_reviews, index=index, num_of_docs=len(prod_reviews), word_doc_frequencies=word_doc_frequencies, num_of_sentences=num_of_sentences)\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print('[RANDOM CHOICE SUMMARY]:')\n",
        "from random import choice\n",
        "prod_reviews_texts = eval(prod_reviews['reviewSentences'][index])\n",
        "range_of_indices = list(range(len(prod_reviews_texts)))\n",
        "summary_sentences = []\n",
        "for i in range(num_of_sentences):\n",
        "    randIdx = choice(range_of_indices)\n",
        "    summary_sentences.append(prod_reviews_texts[randIdx])\n",
        "    range_of_indices.remove(randIdx)\n",
        "\n",
        "for i in summary_sentences:\n",
        "    print(i.strip().capitalize() + \" \")"
      ],
      "metadata": {
        "id": "vkx-FIuWn0MU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}